import numpy as np
# import gym


NUM_EPISODES = 1000

# Neural network parameters
NUM_INPUTS = 1
NUM_OUTPUTS = 1
HIDDEN_LAYER_SIZE = 32
LEARNING_RATE = 1e-4
DISCOUNT_RATE = 0.99
EPSILON = 1
EPSILON_DECAY = 0.99


def relu(layer_outputs):
    """
    "relu" = REctified Linear Unit
    Set all negative layer output values to zero.
    Positive layer output values are left unchanged.

    :param layer_outputs: (np.array) raw output values from a layer
    :return: (np.array) rectified output values
    """
    layer_outputs[layer_outputs < 0] = 0
    return layer_outputs


class DeepQNetwork():

    def __init__(self, num_inputs, num_outputs):
        self.num_inputs = num_inputs
        self.num_outputs = num_outputs
        self.hidden_layer_size = HIDDEN_LAYER_SIZE
        self.learning_rate = LEARNING_RATE
        self.discount_rate = DISCOUNT_RATE
        self.prob_random_action = EPSILON
        self.prob_random_action_decay = EPSILON_DECAY
        self.weights = {
            '1': np.random.randn(self.hidden_layer_size, self.num_inputs) / np.sqrt(self.num_inputs),
            '2': np.random.randn(self.hidden_layer_size, self.num_outputs) / np.sqrt(self.hidden_layer_size)
        }
        self.memory = []

    def forward_pass(self, inputs):
        """
        Pass state vector through network to obtain Q-values.

        :param inputs: (np.array) state vector
        :return: (dict) output values from each neuron in each layer
        """
        layer_outputs = {'1': relu(np.dot(self.weights['1'], inputs))}
        layer_outputs['2'] = np.dot(self.weights['2'], layer_outputs['1'])
        return layer_outputs

    def choose_action(self, q_values):
        """
        Selects either a random action or the action that corresponds
        with the highest Q-value generated by the network.

        :param q_values: (np.array) output layer neuron values
        :return: (int) action index
        """
        if np.random.random() < self.prob_random_action:
            action = np.random.choice(self.num_outputs)
            self.prob_random_action *= self.prob_random_action_decay
        else:
            action = np.argmax(q_values)
        return action

    def remember(self, state, action, reward, done, q_values):
        """
        Store episode info to be used later for back-propagation.

        :param state: (list) state info
        :param action: (int) action index
        :param reward: (int) reward given for the particular action
        :param done: (bool) game over if 'True'
        :param q_values: (np.array) output layer neuron values
        :return: None
        """
        experience = (state, action, reward, done, q_values)
        self.memory.append(experience)

    def replay_memory(self):
        """
        Fetch all information about a particular episode

        :return: (np.array) memory in reverse-chronological order
        """
        return np.array(reversed(self.memory))

    def compute_targets(self, experience, next_q_values):
        """
        Calculate estimates for the target Q-values used to train the network
        in a supervised learning fashion.

        :param experience: (tuple) all information about a particular decision
        :param next_q_values: (tuple) Q-values generated for the next state
        :return: (np.array) target values for each output neuron
        """
        _, action, reward, done, q_values = experience
        targets = np.array(q_values)
        targets[action] = reward + (1 - done) * self.discount_rate * max(next_q_values)
        return targets

    def compute_gradients(self, inputs, layer_outputs, targets):
        """
        Layer output equations:         Layer output derivatives (Eq. 1 - 2):       Layer output derivatives (Eq. 3):
          ~ y1 = max(0, W1 * x),          ~ dy1_dW1 = x if W1 * x >= 0 else 0         ~ N/A
          ~ y2 = W2 * y1,                 ~ dy2_dW2 = y1,                             ~ dy2_dy1 = W2

        Loss function (Eq. 4):
          ~ L(x, W) = 0.5 * (target - y2)^2

        Loss function derivatives w.r.t. layer outputs (Eq. 5 - 6)
          ~ dL_dy2 = target - y2
          ~ dL_dy1 = dL_dy2 * dy2_dy1 = (target - y2) * W2

        Loss function derivatives w.r.t. weights (Eq. 7 - 8)
          ~ dL_dW2 = dL_dy2 * dy2_dW2 = (target - y2) * y1
          ~ dL_dW1 = dL_dy1 * dy1_dW1 = (target - y2) * W2 * (x if W1 * x >= 0 else 0)

        :param targets: (np.array) target values for each output neuron
        :param inputs: (np.array) state vector
        :param layer_outputs: (dict) output values for each neuron in each layer
        :return: (np.array) incremental changes to be applied to the weights of each layer
                 (np.array)
        """
        # Eq. 1 - 2
        grad_layer_outputs_1_wrt_weights_1 = [i if o > 0 else 0 for i, o in zip(inputs, layer_outputs['1'])]
        grad_layer_outputs_2_wrt_weights_2 = layer_outputs['1']

        # Eq. 3
        grad_layer_outputs_2_wrt_layer_outputs_1 = self.weights['2']

        # Eq. 4
        loss = 0.5 * np.square(targets - layer_outputs['2'])

        # Eq. 5 - 6
        grad_loss_wrt_layer_outputs_2 = targets - layer_outputs['2']
        grad_loss_wrt_layer_outputs_1 = np.dot(grad_layer_outputs_2_wrt_layer_outputs_1.T,
                                               grad_loss_wrt_layer_outputs_2)

        # Eq. 7 - 8
        grad_loss_wrt_weights_2 = np.dot(grad_loss_wrt_layer_outputs_2.T,
                                         grad_layer_outputs_2_wrt_weights_2)
        grad_loss_wrt_weights_1 = np.dot(grad_loss_wrt_layer_outputs_1,
                                         grad_layer_outputs_1_wrt_weights_1)
        weight_updates = {'1': grad_loss_wrt_weights_1,
                          '2': grad_loss_wrt_weights_2
                          }
        return weight_updates, loss

    def backward_pass(self, weight_updates):
        """
        Propagates backwards through the network to update the weights
        in the direction that minimizes the loss function.

        :param weight_updates: (dict) updates to be applied to each weight in each layer
        :return: None
        """
        self.weights['1'] += self.learning_rate * weight_updates['1']
        self.weights['2'] += self.learning_rate * weight_updates['2']

env = gym.make()
deep_q_network = DeepQNetwork(NUM_INPUTS, NUM_OUTPUTS)

for i in range(NUM_EPISODES):
    state = env.reset()
    done = False

    while done is not True:
        q_values = deep_q_network.forward_pass(state)
        action = deep_q_network.choose_action(q_values)
        next_state, reward, done, info = env.step(action)
        deep_q_network.remember(state, action, reward, done, q_values)

    experiences = deep_q_network.replay_memory()
    next_q_values = [0 for _ in range(NUM_OUTPUTS)]

    for experience in experiences:
        state, _, _, _, q_values = experience
        targets = deep_q_network.compute_targets(experience, next_q_values)
        weight_updates, loss = deep_q_network.compute_gradients(state, q_values, targets)
        deep_q_network.backward_pass(weight_updates)
        next_q_values = q_values
